{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\" >\n",
    "<h1 style=\"margin-top: 0.2em; margin-bottom: 0.1em;\">Exception Handling</h1>\n",
    "</div>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7984816",
   "metadata": {},
   "source": [
    "Until now you have probably strumbled across several **error messages** when you wrote Python code. In general, these error messages are divided into two categories: \n",
    "\n",
    "- (**semantic errors** are errors, where your code does run and produces an result, but it is not the result you've been looking for due to i.e. taking the wrong data or function)\n",
    "- First, there are **syntax errors**, which indicate that at some point in your code you used an **invalid command**, e.g. you forgot an indent or wrote a colon too much. The interpreter checks for these syntax errors before you code is actually executed. But we do not want to go into detail here. \n",
    "- Instead, we want to discuss the second category of error messages. These error messages are problems which the interpreter encounters when it actually executes your code. These errors are also called **exceptions**. By default, they are **fatal** and stop your program immediately when the exception occurs. \n",
    "\n",
    "In the following is a list of common exceptions: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e78c06",
   "metadata": {},
   "source": [
    "| Exception | Cause |\n",
    "| -------- | ------- |\n",
    "| Attribute Error | Raised when attribute assignment or reference fails |\n",
    "| Import Error | Raised when the imported module is not found |\n",
    "| Index Error | Raised when index of a sequence is out of range |\n",
    "| KeyError | Raised when a key is not found in a dictionary | \n",
    "| Keyboard Interrupt | Raised when the user hits interrupt key(Ctrl + C or Delete) |\n",
    "| Memory Error | Raised when an operation runs out of memory | \n",
    "| Name Error | Raised when a variable is not found in local or global scope | \n",
    "| Syntax Error | Raised by parser when syntax error is encountered |\n",
    "| IndentationError | Raised when there is incorrect indentation | \n",
    "| Type Error | Raised when a function or operation is applied to an object of incorrect type | \n",
    "| Value Error | Raised when a function gets argument of correct type but improper value |\n",
    "| Zero Division Error | Raised when second operand of division or modulo operation is zero |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8a934d",
   "metadata": {},
   "source": [
    "If you are working with data streams, e.g. from websites and APIs, it is advisable that you take certain **errors** into **account** such that not the whole program aborts because of an **unimportant detail** in the data stream. Besides the data stream itself, there is an endless number of potential causes for errors. \n",
    "\n",
    "In these cases, we wrap our code with a `try` **statement**, and catch a possible **exception** with an `except` statement. You can also catch **multiple exceptions** at the same time by adding underneath more `except` statements. If you want to have the respective **message** of the exception available in the `exception` block give it a **variable name**, like in the `with` statement. If you use a `finally` **statement** at the end of your `try` statement, the **clause** inside the `finally` statement will be **executed last**, whether or not the `try` statement raised an exception. \n",
    "\n",
    "Let us **catch** some trivial **exceptions**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68c0224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# error prone code\n",
    "size = len(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ae2bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # error prone code\n",
    "    size = len(x)\n",
    "except NameError as e:\n",
    "    # report name error\n",
    "    print(f\"Got error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a73120",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    # error prone code\n",
    "    y = 12\n",
    "    size = len(y)\n",
    "except NameError as e:\n",
    "    # report name error\n",
    "    print(f\"Got name error: {e}\")\n",
    "except TypeError as e:\n",
    "    # report type error\n",
    "    print(f\"Got type error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394698e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    # error prone code\n",
    "    size = len(z)\n",
    "except NameError as e:\n",
    "    # report name error\n",
    "    print(f\"Got name error: {e}\")\n",
    "except TypeError as e:\n",
    "    # report type error\n",
    "    print(f\"Got type error: {e}\")\n",
    "finally:\n",
    "    # report finished block\n",
    "    print(\"Finished try block\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c18968",
   "metadata": {},
   "source": [
    "Please keep in mind, however, that you should not **abuse** `try` statements to make **poor code** run, but only to deal with **unavoidable problems**. This is also the reason why we have not introduced exception handling earlier on. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f06880b-f8a3-4c51-adfd-47ddc219b5f5",
   "metadata": {},
   "source": [
    "***\n",
    "<div style=\"text-align: center;\" >\n",
    "<h1 style=\"margin-top: 0.2em; margin-bottom: 0.1em;\">Sentiment Analysis</h1>\n",
    "<h4 style=\"margin-top: 0.7em; margin-bottom: 0.3em; font-style:italic\">Ey you?! Yes, you! I am your mother!</h4>\n",
    "</div>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa50ff2-afe9-40cf-b1e1-76e9147a77aa",
   "metadata": {},
   "source": [
    "Today's topic is Sentiment Analysis. This is the approach of using natural language processing and text analysis to quantitatively analyze the sentiment of given documents. You can think of it as giving a piece of text or multiple texts to someone and asking them to tell you which emotions are conveyed in the given text(s) except that you are not giving the text(s) to a human but to a machine.<br>\n",
    "At the basic level this analysis is only about detecting positive, negative, or neutral sentimental valence. An example for this rather simple method of sentiment analysis would be the VADER (Valence Aware Dictionary and sEntiment Reasoner) sentiment analyzer which will be introduced further down.<br>\n",
    "Something a bit more advanced would be the identification of specific emotions like anger, hate, sadness, or joy. This can be done with LEIA (Linguistic Embeddings for the Identification of Affect)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe0e74b-8137-4a58-86de-033e2ae82b8c",
   "metadata": {},
   "source": [
    "Now, you might ask why sentiment analysis might be important. Simple example: Think of product reviews... Nobody wants to read hundreds, thousands of product reviews and keep track of whether people were sad, angry, joyful, or simply neutral about a product. Imagine all the hate you would be subjected to reading this stuff...<br>\n",
    "So, the solution is pretty simple: Just have your machine do the 'reading'!<br>\n",
    "Enough talk, let's get down to some analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9411ec-5a92-4cca-be62-ea3f9c7c6947",
   "metadata": {},
   "source": [
    "***\n",
    "# VADER\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1080d2ba-e233-4c7f-996f-393f3347c429",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sentiment analysis part\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2237ef-9679-4077-974a-2427f62e926e",
   "metadata": {},
   "source": [
    "First we load in the sentiment analyzer. For this we have to download the specific analyzer we wish to use. In our case this is done with `nltk.download('vader_lexicon')`. You don't have to do this every time just once before using the analyzer for the first time. We then import the `SentimentIntensityAnalyzer` class which we will need to perform the actual sentiment analysis.<br>\n",
    "If you have problems at any point and the explanation given here does not do it for you just check this [link](https://www.nltk.org/howto/sentiment.html) for a tutorial on how to use the nltk VADER sentiment analyzer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd9fa02-927e-4eed-be9b-2453966e0eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SentimentIntensityAnalyzer() # Create a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304523b9-9c72-4542-8314-d0ff42e156fb",
   "metadata": {},
   "source": [
    "We create an object of the class `SentimentIntesityAnalyzer`. This is used to perform the sentiment analysis on individual texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9aa9ef2-b67b-4735-a38c-6d55cf52d2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['I hate pineapple!',\n",
    "        'I am not really sure what to make of our new prime minister',\n",
    "        'Last night was so hilarious! I wish you would have been there with us...',\n",
    "        'I do not hate homework',\n",
    "        'I love my data',\n",
    "        'Sometimes I feel down when I think about the state of our planet', \n",
    "        'When people say \"Eat the rich\" I am never quite certain whether they really mean to eat them...',\n",
    "        \"Seeing Aragorn completely murder Sauron's forces in The Lord of the Rings brought tears of joy to my face!\"]\n",
    "\n",
    "df = pd.DataFrame(data, columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba5ec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b4bb6a-e816-40b2-b8fd-8a7d4df089a4",
   "metadata": {},
   "source": [
    "Next, we need to get our text data into a format that can be analyzed by the classifier. One option would be to create a list of separate texts and feed this to the classifier. Another option would be to input a column of a pandas data frame. Let's have a look at the output:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140d1dcc-4fe6-4d91-b8d4-5558edd703eb",
   "metadata": {},
   "source": [
    "## Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db389057-8829-40ec-978c-dee5136caa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data: # using a list\n",
    "    print(i, '\\n', classifier.polarity_scores(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d96ac24-b7b2-49df-bc4c-47937e7ebea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df['text']: # using a df column\n",
    "    print(i, '\\n', classifier.polarity_scores(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4430a18-6919-4726-91fa-57e1595cf9e8",
   "metadata": {},
   "source": [
    "As you see the classifier assigned each of our example texts four different scores. A 'neg', 'neu', 'pos', and 'compound' score. You might already have guessed what the first three scores stand for. They hold the information about how negative, neutral, or positive a given text was classified to be. The fourth score, the compound score, is a combination of those three other scores. For general purposes you might consider a negative compound score to indicate an overall negative text, a positive compound score indicates a positive text, and a compound score of 0 indicates a neutral text. ***Note that this definition might vary depending on your specific use case.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1002f2-8eab-4051-8cfe-0b46af0bc918",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43694029-b8ac-41b0-a3a3-4100087ce05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1823f37d-f1fd-425a-a50e-d9f64724ec14",
   "metadata": {},
   "source": [
    "Ok, now that you have seen VADER in action let's talk about evaluating our classifications. Take some time and create a [heatmap](https://seaborn.pydata.org/generated/seaborn.heatmap.html) showing the real and predicted sentiments of our example texts. ***Hint: The cell above tells you which libraries you might want to use for the task. The cell below hints at what you might be missing as of now.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cead0c2-0da4-4b0d-8596-461464b1cb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = ['negative',\n",
    "              'neutral',\n",
    "              'positive',\n",
    "              'neutral',\n",
    "              'positive',\n",
    "              'negative',\n",
    "              'neutral',\n",
    "              'positive']\n",
    "\n",
    "df['annotation'] = annotations\n",
    "df['prediction'] = None\n",
    "\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1c07b4-2827-4ab1-8001-0178b9a25918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time for action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b8dd5e-05db-47b1-9539-34f186cae7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "comps = [] # empty list to store individual compound scores\n",
    "\n",
    "for i in df['text']: # run VADER and extract specific compound scores\n",
    "    comps.append(classifier.polarity_scores(i)['compound'])\n",
    "\n",
    "df['compound'] = comps # create a column containing the compound scores\n",
    "\n",
    "df.loc[df['compound'] < 0, 'prediction'] = 'negative' # depending on compound score assign label\n",
    "df.loc[df['compound'] == 0, 'prediction'] = 'neutral'\n",
    "df.loc[df['compound'] > 0, 'prediction'] = 'positive'\n",
    "\n",
    "matrix = confusion_matrix(df['annotation'], df['prediction']) # create a confusion matrix of true/predicted labels\n",
    "\n",
    "heatmap = sns.heatmap(matrix, annot = True,  cmap = \"Greens\") # plot the results\n",
    "heatmap.set_title(\"Heatmap Showing Annotation Results\\n\");\n",
    "heatmap.xaxis.set_ticklabels(['negative', 'neutral', 'positive'])\n",
    "heatmap.yaxis.set_ticklabels(['negative', 'neutral', 'positive'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d91938-fe55-4669-8ea5-c3986b1677ad",
   "metadata": {},
   "source": [
    "One can see that the predictions and real labels did not match well. In a perfect scenario we would observe a deeply colored diagonal from the top left to the bottom right. This would indicate that predictions and true labels (nearly) always matched.<br>\n",
    "The y-axis shows the true labels of the texts while the x-axis shows the predicted labels. Inspecting the above plot we see that the sentiment classification was not too successful in this example. ***Keep in mind though that we only had a sample of 8 different texts and that the 'true' labels were assigned by one human. Maybe different annotators would classify the texts differently...***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce66fb2-8ac8-40aa-81ac-965cc451d40e",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e97d29a-4460-44b5-8b84-98c934ebfe3b",
   "metadata": {},
   "source": [
    "Now that we have applied the VADER sentiment analyzer to some texts let's think about possible strengths and weaknesses of our approach. Take a few minutes an try to come up with some pros and cons:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaa7e7e-9a47-43d3-baa1-141163d93bef",
   "metadata": {},
   "source": [
    "Cons:\n",
    "- Can't extract subtle emotional valence\n",
    "- Unaware of context ('I do not hate' -> negative)\n",
    "- Gives only negative, neutral, or positive labels (in some cases e.g product reviews we already have this info (stars))\n",
    "\n",
    "Pros:\n",
    "- Easy to use\n",
    "- Easy to understand\n",
    "- (Can be) fast\n",
    "\n",
    "***Note that this list is not exhaustive.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31534a97-65fb-4431-9032-43f444f5c451",
   "metadata": {},
   "source": [
    "***\n",
    "# Qick, you need to be my young padawan?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdd4b38-65e8-430e-998b-8d25f65bf3ab",
   "metadata": {},
   "source": [
    "## Parallelism\n",
    "\n",
    "As of now we have conducted the sentiment analysis on a very small sample of text data only. In pretty much every other scenario you will have to apply the presented method to more than just a few lines of text. Once you do this you will notice that the time it takes for VADER to classify all of your data is going to significantly increase. In order to counter this you can use the [multiprocessing](https://docs.python.org/3/library/multiprocessing.html) library to parallelize the process of sentiment classification. This way your machine will classify multiple texts at once instead of after each other which in turn will significantly reduce your runtime. Of course this heavily depends on the machine you are using. A potato won't be especially fast no matter how well you parallelize your task...<br>\n",
    "***More on parallelism now***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
